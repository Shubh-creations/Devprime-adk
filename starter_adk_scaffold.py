benchmark_adk/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â”œâ”€â”€ ppo_agent.py
â”‚   â””â”€â”€ model.py
â”‚
â”œâ”€â”€ environment/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gridworld_env.py
â”‚   â”œâ”€â”€ robot_env.py
â”‚   â””â”€â”€ wrappers.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ visualizer.py
â”‚   â””â”€â”€ replay_buffer.py
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ gridworld_config.yaml
â”‚   â”œâ”€â”€ robot_config.yaml
â”‚   â””â”€â”€ train_config.yaml
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ gridworld_benchmark.py
â”‚   â””â”€â”€ robot_benchmark.py
â”‚
â”œâ”€â”€ train.py
â”œâ”€â”€ evaluate.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ STORY.md

# STORY.md
## Inspiration

Our inspiration came from a gap we noticed in lightweight yet robust reinforcement learning (RL) toolkits that balance educational clarity with research-grade capabilities. We wanted a framework that could scale from simple GridWorld simulations to robotic environments, all while supporting benchmarking and reproducibility.

## What it does

Benchmark-ADK allows researchers and developers to train, evaluate, and benchmark RL agents using modular, reproducible tools. It supports DQN and PPO agents, gym-compatible environments, experiment scripts, and a training pipeline compatible with real-time and simulated systems.

## How we built it

We started by sketching out the desired structure: agents, environments, utilities, and config layers. We implemented modular agents using PyTorch, environment interfaces using OpenAI Gym conventions, and YAML-based configuration loading. For benchmarking, we added evaluation scripts, plotting utilities, and GitHub Actions-based CI/CD.

## Challenges we ran into

- Making the environment API generic enough to support both grid simulations and robotic agents
- Deciding how to abstract agent models so users could extend easily
- Balancing simplicity with extensibility for new algorithms like PPO
- Keeping the training pipeline modular while still being intuitive

## Accomplishments that we're proud of

- Designing a clean, extensible codebase
- Making it runnable on both local laptops and cloud instances
- Implementing two working agent architectures
- Setting up automated testing with GitHub Actions

## What we learned

- The value of strict modular design in RL pipelines
- How to implement custom gym environments and wrappers
- Best practices for reproducible experiments
- Managing trade-offs between flexibility and clarity

## What's next for Devprime

- Add support for more algorithms like A3C and SAC
- Integrate real robot simulators (Webots, PyBullet, Isaac Sim)
- Include Jupyter dashboards for real-time metrics
- Host benchmark results online and enable plug-and-play experiment uploads

## Built With

- Python â€“ Core language for development
- PyTorch â€“ Deep learning framework for agent models
- OpenAI Gym â€“ Standardized interface for reinforcement learning environments
- NumPy & Pandas â€“ Numerical processing and data analysis
- Matplotlib & Seaborn â€“ Visualization of training metrics and performance
- YAML â€“ Configurable experiment settings
- GitHub Actions â€“ CI/CD workflows for testing and validation
- Jupyter Notebooks (planned) â€“ Interactive analysis and visualization
- (Optional Extensions)
- Webots / Isaac Sim / PyBullet â€“ For real-time robotic simulations
- Hydra / Argparse â€“ For scalable CLI-based configuration
- TensorBoard â€“ Model training logs and comparison dashboards

## Try It Out / See the Code

- ğŸ”— **GitHub Repository**: [github.com/Shubh-creations/benchmark-adk](https://github.com/Shubh-creations/benchmark-adk)
- â–¶ï¸ **Try on Google Colab**: [Run a demo notebook](https://colab.research.google.com/github/Shubh-creations/benchmark-adk/blob/main/notebooks/gridworld_demo.ipynb)
- ğŸ§ª **Run Benchmark Locally**:
  ```bash
  git clone https://github.com/Shubh-creations/benchmark-adk.git
  cd benchmark-adk
  pip install -r requirements.txt
  python train.py --config config/gridworld_config.yaml
  ```
